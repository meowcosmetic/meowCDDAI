"""
SAM + CLIP Object Detection Test
S·ª≠ d·ª•ng Segment Anything Model (SAM) + CLIP ƒë·ªÉ detect objects d·ª±a tr√™n image embedding

C√°ch ho·∫°t ƒë·ªông:
1. Load ·∫£nh m·∫´u (reference image) c·ªßa object c·∫ßn t√¨m (v√≠ d·ª•: c√¢y b√∫t)
2. T√≠nh CLIP embedding c·ªßa ·∫£nh m·∫´u
3. Segment ·∫£nh target v·ªõi SAM ƒë·ªÉ t√¨m t·∫•t c·∫£ c√°c objects
4. T√≠nh CLIP embedding c·ªßa m·ªói segment
5. So s√°nh similarity gi·ªØa reference embedding v√† segment embeddings
6. Return c√°c segments c√≥ similarity cao (match v·ªõi object m·∫´u)

∆Øu ƒëi·ªÉm:
- Detect ƒë∆∞·ª£c objects ngay c·∫£ khi b·ªã che khu·∫•t m·ªôt ph·∫ßn
- Kh√¥ng c·∫ßn dataset training
- Ch·ªâ c·∫ßn ·∫£nh m·∫´u c·ªßa object c·∫ßn t√¨m
"""

import os
import cv2
import numpy as np
import torch
from PIL import Image
from typing import List, Dict, Tuple, Optional, Union
from pathlib import Path
import logging

logger = logging.getLogger(__name__)

# Lazy imports
try:
    from ultralytics import SAM, FastSAM
    SAM_AVAILABLE = True
except ImportError:
    SAM_AVAILABLE = False
    logger.warning("[SAM+CLIP] Ultralytics kh√¥ng ƒë∆∞·ª£c c√†i ƒë·∫∑t. C√†i: pip install ultralytics>=8.0.0")

try:
    import clip
    CLIP_AVAILABLE = True
except ImportError:
    CLIP_AVAILABLE = False
    logger.warning("[SAM+CLIP] CLIP kh√¥ng ƒë∆∞·ª£c c√†i ƒë·∫∑t. C√†i: pip install git+https://github.com/openai/CLIP.git")

try:
    from ultralytics.nn.text_model import CLIP as UltralyticsCLIP
    ULTRALYTICS_CLIP_AVAILABLE = True
except ImportError:
    ULTRALYTICS_CLIP_AVAILABLE = False


class SAMCLIPDetector:
    """
    SAM + CLIP Object Detector
    
    Detect objects trong ·∫£nh b·∫±ng c√°ch:
    1. Segment v·ªõi SAM
    2. Match v·ªõi reference image embedding b·∫±ng CLIP
    """
    
    def __init__(
        self,
        sam_model: str = "sam_b.pt",
        clip_model: str = "ViT-B/32",
        use_fastsam: bool = False,
        device: Optional[str] = None,
        similarity_threshold: float = 0.25
    ):
        """
        Args:
            sam_model: SAM model name ho·∫∑c path (sam_b.pt, sam_l.pt, sam_x.pt)
            clip_model: CLIP model name (ViT-B/32, ViT-L/14, etc.)
            use_fastsam: S·ª≠ d·ª•ng FastSAM thay v√¨ SAM (nhanh h∆°n nh∆∞ng k√©m ch√≠nh x√°c h∆°n)
            device: Device ƒë·ªÉ ch·∫°y model ('cuda', 'cpu', ho·∫∑c None ƒë·ªÉ auto-detect)
            similarity_threshold: Ng∆∞·ª°ng similarity ƒë·ªÉ coi l√† match (0-1)
        """
        self.sam_model_name = sam_model
        self.clip_model_name = clip_model
        self.use_fastsam = use_fastsam
        self.similarity_threshold = similarity_threshold
        
        # Auto-detect device
        if device is None:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device
        
        # Log GPU info
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB
            logger.info(f"[SAM+CLIP] üöÄ GPU detected: {gpu_name} ({gpu_memory:.1f} GB)")
        else:
            logger.info(f"[SAM+CLIP] ‚ö†Ô∏è  GPU kh√¥ng available, s·ª≠ d·ª•ng CPU (s·∫Ω ch·∫≠m h∆°n)")
        
        logger.info(f"[SAM+CLIP] Initializing on device: {self.device}")
        
        # Initialize SAM
        self.sam = None
        if SAM_AVAILABLE:
            try:
                # Ultralytics SAM/FastSAM t·ª± ƒë·ªông detect GPU, nh∆∞ng c√≥ th·ªÉ ch·ªâ ƒë·ªãnh device
                # Th·ª≠ set device n·∫øu c√≥ GPU
                if use_fastsam:
                    # FastSAM c√≥ th·ªÉ nh·∫≠n device parameter
                    try:
                        self.sam = FastSAM(sam_model)
                        # Set device n·∫øu c√≥ th·ªÉ
                        if hasattr(self.sam, 'to') and self.device != "cpu":
                            try:
                                self.sam.to(self.device)
                            except:
                                pass
                    except:
                        self.sam = FastSAM(sam_model)
                    
                    # Check SAM device
                    sam_device = "cpu"
                    if hasattr(self.sam, 'device'):
                        sam_device = str(self.sam.device)
                    elif hasattr(self.sam, 'model'):
                        if hasattr(self.sam.model, 'device'):
                            sam_device = str(self.sam.model.device)
                        elif hasattr(self.sam.model, 'parameters'):
                            sam_device = str(next(self.sam.model.parameters()).device)
                    logger.info(f"[SAM+CLIP] ‚úÖ FastSAM loaded: {sam_model}")
                    logger.info(f"[SAM+CLIP]    Device: {sam_device}")
                else:
                    self.sam = SAM(sam_model)
                    # Set device n·∫øu c√≥ th·ªÉ
                    if hasattr(self.sam, 'to') and self.device != "cpu":
                        try:
                            self.sam.to(self.device)
                        except:
                            pass
                    
                    # Check SAM device
                    sam_device = "cpu"
                    if hasattr(self.sam, 'device'):
                        sam_device = str(self.sam.device)
                    elif hasattr(self.sam, 'model'):
                        if hasattr(self.sam.model, 'device'):
                            sam_device = str(self.sam.model.device)
                        elif hasattr(self.sam.model, 'parameters'):
                            sam_device = str(next(self.sam.model.parameters()).device)
                    logger.info(f"[SAM+CLIP] ‚úÖ SAM loaded: {sam_model}")
                    logger.info(f"[SAM+CLIP]    Device: {sam_device}")
            except Exception as e:
                logger.error(f"[SAM+CLIP] ‚ùå Kh√¥ng th·ªÉ load SAM model: {str(e)}")
                raise
        
        # Initialize CLIP
        self.clip_model = None
        self.clip_preprocess = None
        self.clip_type = None  # 'openai' or 'ultralytics'
        
        # Th·ª≠ load Ultralytics CLIP tr∆∞·ªõc (th∆∞·ªùng c√≥ s·∫µn v·ªõi ultralytics)
        if ULTRALYTICS_CLIP_AVAILABLE:
            try:
                self.clip_model = UltralyticsCLIP(clip_model, device=self.device)
                self.clip_type = 'ultralytics'
                logger.info(f"[SAM+CLIP] ‚úÖ Ultralytics CLIP loaded: {clip_model} (device: {self.device})")
            except Exception as e:
                logger.warning(f"[SAM+CLIP] Kh√¥ng th·ªÉ load Ultralytics CLIP: {str(e)}")
        
        # N·∫øu Ultralytics CLIP kh√¥ng available, th·ª≠ OpenAI CLIP
        if self.clip_model is None and CLIP_AVAILABLE:
            try:
                self.clip_model, self.clip_preprocess = clip.load(clip_model, device=self.device)
                self.clip_model.eval()
                self.clip_type = 'openai'
                # Check CLIP device
                clip_device = next(self.clip_model.parameters()).device if hasattr(self.clip_model, 'parameters') else self.device
                logger.info(f"[SAM+CLIP] ‚úÖ OpenAI CLIP loaded: {clip_model} (device: {clip_device})")
            except Exception as e:
                logger.warning(f"[SAM+CLIP] Kh√¥ng th·ªÉ load OpenAI CLIP: {str(e)}")
        
        # N·∫øu c·∫£ hai ƒë·ªÅu kh√¥ng available
        if self.clip_model is None:
            logger.error("[SAM+CLIP] ‚ùå CLIP kh√¥ng available!")
            logger.error("[SAM+CLIP]    ƒê·ªÉ s·ª≠ d·ª•ng classification, c√†i ƒë·∫∑t m·ªôt trong c√°c c√°ch sau:")
            logger.error("[SAM+CLIP]    1. pip install git+https://github.com/openai/CLIP.git")
            logger.error("[SAM+CLIP]    2. Ho·∫∑c ƒë·∫£m b·∫£o ultralytics>=8.0.0 ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t")
            logger.warning("[SAM+CLIP]    Classification s·∫Ω b·ªã t·∫Øt, ch·ªâ c√≥ th·ªÉ detect objects kh√¥ng c√≥ t√™n")
        
        # Cache ƒë·ªÉ l∆∞u reference embeddings
        self.reference_embeddings: Dict[str, torch.Tensor] = {}
        
        # Common object classes ƒë·ªÉ ph√¢n lo·∫°i
        self.common_classes = [
            "pen", "pencil", "book", "cup", "bottle", "glass", "bowl", "plate",
            "phone", "cell phone", "mobile phone", "laptop", "computer", "tablet",
            "mouse", "keyboard", "remote control",
            "toy", "doll", "teddy bear", "ball", "building blocks",
            "chair", "table", "desk", "bed", "couch", "sofa",
            "scissors", "knife", "fork", "spoon",
            "clock", "watch", "vase", "flower", "plant",
            "apple", "banana", "orange", "sandwich", "pizza", "food",
            "shirt", "pants", "shoe", "hat", "dress",
            "car", "bicycle", "motorcycle", "vehicle",
            "dog", "cat", "bird", "horse", "animal",
            "person", "child", "adult", "baby",
            "hand", "finger", "arm", "leg"
        ]
    
    def _load_image(self, image_path: Union[str, Path, np.ndarray]) -> Tuple[np.ndarray, Image.Image]:
        """
        Load image t·ª´ path ho·∫∑c numpy array
        
        Returns:
            (numpy_image, PIL_image)
        """
        if isinstance(image_path, (str, Path)):
            # Convert to Path object
            image_path_orig = Path(image_path)
            image_path = image_path_orig
            
            # N·∫øu path kh√¥ng t·ªìn t·∫°i v√† l√† relative path, th·ª≠ t√¨m trong c√°c th∆∞ m·ª•c c√≥ th·ªÉ
            if not image_path.exists() and not image_path.is_absolute():
                # Danh s√°ch c√°c th∆∞ m·ª•c ƒë·ªÉ th·ª≠ t√¨m
                search_dirs = []
                
                # 1. Th∆∞ m·ª•c "test SAM" (t·ª´ v·ªã tr√≠ file hi·ªán t·∫°i)
                try:
                    if '__file__' in globals():
                        test_sam_dir = Path(__file__).parent
                        search_dirs.append(test_sam_dir)
                except:
                    pass
                
                # 2. Th∆∞ m·ª•c "test SAM" t·ª´ working directory
                test_sam_dir_cwd = Path.cwd() / "test SAM"
                if test_sam_dir_cwd.exists():
                    search_dirs.append(test_sam_dir_cwd)
                
                # 3. Th∆∞ m·ª•c hi·ªán t·∫°i
                search_dirs.append(Path.cwd())
                
                # Th·ª≠ t√¨m trong c√°c th∆∞ m·ª•c
                found = False
                for search_dir in search_dirs:
                    test_path = search_dir / image_path_orig
                    if test_path.exists():
                        image_path = test_path
                        logger.info(f"[SAM+CLIP] Found image: {image_path}")
                        found = True
                        break
                
                if not found:
                    searched_paths = [str(d / image_path_orig) for d in search_dirs]
                    raise FileNotFoundError(
                        f"Kh√¥ng t√¨m th·∫•y ·∫£nh: {image_path_orig}\n"
                        f"ƒê√£ th·ª≠ t√¨m trong:\n" + "\n".join(f"  - {p}" for p in searched_paths)
                    )
            
            # Load image
            numpy_image = cv2.imread(str(image_path))
            if numpy_image is None:
                raise ValueError(f"Kh√¥ng th·ªÉ ƒë·ªçc ·∫£nh: {image_path}")
            pil_image = Image.open(image_path).convert("RGB")
        elif isinstance(image_path, np.ndarray):
            numpy_image = image_path
            if len(numpy_image.shape) == 3 and numpy_image.shape[2] == 3:
                # BGR to RGB
                pil_image = Image.fromarray(cv2.cvtColor(numpy_image, cv2.COLOR_BGR2RGB))
            else:
                pil_image = Image.fromarray(numpy_image)
        else:
            raise ValueError(f"Unsupported image type: {type(image_path)}")
        
        return numpy_image, pil_image
    
    def compute_clip_embedding(self, image: Union[str, Path, np.ndarray, Image.Image]) -> torch.Tensor:
        """
        T√≠nh CLIP embedding c·ªßa ·∫£nh
        
        Args:
            image: Image path, numpy array, ho·∫∑c PIL Image
        
        Returns:
            CLIP embedding tensor (normalized)
        """
        # Convert to PIL Image
        if isinstance(image, (str, Path)):
            pil_image = Image.open(image).convert("RGB")
        elif isinstance(image, np.ndarray):
            if len(image.shape) == 3 and image.shape[2] == 3:
                pil_image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
            else:
                pil_image = Image.fromarray(image)
        elif isinstance(image, Image.Image):
            pil_image = image.convert("RGB")
        else:
            raise ValueError(f"Unsupported image type: {type(image)}")
        
        # Compute embedding
        if hasattr(self.clip_model, 'encode_image'):
            # Ultralytics CLIP
            image_tensor = self.clip_model.image_preprocess(pil_image).unsqueeze(0).to(self.device)
            with torch.no_grad():
                embedding = self.clip_model.encode_image(image_tensor)
        else:
            # OpenAI CLIP
            image_tensor = self.clip_preprocess(pil_image).unsqueeze(0).to(self.device)
            with torch.no_grad():
                embedding = self.clip_model.encode_image(image_tensor)
        
        # Normalize embedding
        embedding = embedding / embedding.norm(dim=-1, keepdim=True)
        
        return embedding.squeeze(0)
    
    def register_reference_image(
        self,
        image_path: Union[str, Path, np.ndarray, Image.Image],
        object_name: str = "object"
    ) -> None:
        """
        ƒêƒÉng k√Ω ·∫£nh m·∫´u (reference image) ƒë·ªÉ t√¨m ki·∫øm
        
        Args:
            image_path: Path ƒë·∫øn ·∫£nh m·∫´u ho·∫∑c image array
            object_name: T√™n object (ƒë·ªÉ cache embedding)
        """
        logger.info(f"[SAM+CLIP] Registering reference image: {object_name}")
        embedding = self.compute_clip_embedding(image_path)
        self.reference_embeddings[object_name] = embedding
        logger.info(f"[SAM+CLIP] ‚úÖ Reference embedding computed for: {object_name}")
    
    def segment_image(self, image: Union[str, Path, np.ndarray]) -> List[Dict]:
        """
        Segment ·∫£nh v·ªõi SAM ƒë·ªÉ t√¨m t·∫•t c·∫£ c√°c objects
        
        Args:
            image: Image path ho·∫∑c numpy array
        
        Returns:
            List of segments v·ªõi format:
            {
                'mask': np.ndarray,  # Binary mask
                'bbox': [x, y, w, h],  # Bounding box
                'area': int,  # Pixel area
                'confidence': float  # Confidence score
            }
        """
        if self.sam is None:
            raise RuntimeError("SAM model ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o")
        
        # Load image
        numpy_image, pil_image = self._load_image(image)
        
        # Run SAM segmentation
        # FastSAM t·ª± ƒë·ªông segment t·∫•t c·∫£ objects
        # SAM c·∫ßn prompts, nh∆∞ng c√≥ th·ªÉ d√πng auto-annotation
        if self.use_fastsam:
            # FastSAM: predict without prompts ƒë·ªÉ segment t·∫•t c·∫£
            results = self.sam.predict(pil_image, imgsz=1024)
        else:
            # SAM: c·∫ßn prompts, nh∆∞ng c√≥ th·ªÉ d√πng grid points
            # T·∫°o grid points ƒë·ªÉ segment nhi·ªÅu objects
            h, w = numpy_image.shape[:2]
            grid_points = []
            step = 100  # Grid step size
            for y in range(step, h, step):
                for x in range(step, w, step):
                    grid_points.append([x, y])
            
            results = self.sam.predict(pil_image, points=grid_points, imgsz=1024)
        
        # Extract segments
        segments = []
        if len(results) > 0:
            result = results[0]
            if hasattr(result, 'masks') and result.masks is not None:
                masks = result.masks.data.cpu().numpy() if hasattr(result.masks, 'data') else result.masks
                
                for i, mask in enumerate(masks):
                    # Convert mask to binary
                    if mask.dtype != np.uint8:
                        mask = (mask > 0.5).astype(np.uint8) * 255
                    
                    # Get bounding box
                    y_indices, x_indices = np.where(mask > 0)
                    if len(x_indices) > 0 and len(y_indices) > 0:
                        x_min, x_max = int(x_indices.min()), int(x_indices.max())
                        y_min, y_max = int(y_indices.min()), int(y_indices.max())
                        bbox = [x_min, y_min, x_max - x_min, y_max - y_min]
                        
                        # Calculate area
                        area = int(np.sum(mask > 0))
                        
                        # Filter out very small segments (noise)
                        min_area = (h * w) * 0.001  # T·ªëi thi·ªÉu 0.1% di·ªán t√≠ch ·∫£nh
                        if area >= min_area:
                            segments.append({
                                'mask': mask,
                                'bbox': bbox,
                                'area': area,
                                'confidence': 1.0,  # SAM kh√¥ng c√≥ confidence score
                                'segment_id': i
                            })
        
        logger.info(f"[SAM+CLIP] Segmented {len(segments)} objects")
        return segments
    
    def classify_object(
        self,
        image_crop: Union[np.ndarray, Image.Image],
        class_names: Optional[List[str]] = None
    ) -> Tuple[str, float]:
        """
        Ph√¢n lo·∫°i object trong image crop b·∫±ng CLIP
        
        Args:
            image_crop: Cropped image c·ªßa object
            class_names: List c√°c class names ƒë·ªÉ ph√¢n lo·∫°i, None = d√πng common_classes
        
        Returns:
            (class_name, confidence): T√™n class v√† confidence score
        """
        if self.clip_model is None:
            logger.warning("[SAM+CLIP] CLIP model is None, cannot classify")
            return "unknown", 0.0
        
        if class_names is None:
            class_names = self.common_classes
        
        try:
            # Convert to PIL Image n·∫øu c·∫ßn
            if isinstance(image_crop, np.ndarray):
                # ƒê·∫£m b·∫£o c√≥ k√≠ch th∆∞·ªõc h·ª£p l·ªá
                if image_crop.size == 0 or len(image_crop.shape) < 2:
                    return "unknown", 0.0
                
                if len(image_crop.shape) == 3 and image_crop.shape[2] == 3:
                    pil_image = Image.fromarray(cv2.cvtColor(image_crop, cv2.COLOR_BGR2RGB))
                else:
                    pil_image = Image.fromarray(image_crop)
            else:
                pil_image = image_crop.convert("RGB")
            
            # Resize n·∫øu qu√° nh·ªè ho·∫∑c qu√° l·ªõn
            min_size = 32
            max_size = 512
            w, h = pil_image.size
            if min(w, h) < min_size:
                scale = min_size / min(w, h)
                new_w, new_h = int(w * scale), int(h * scale)
                pil_image = pil_image.resize((new_w, new_h), Image.Resampling.LANCZOS)
            elif max(w, h) > max_size:
                scale = max_size / max(w, h)
                new_w, new_h = int(w * scale), int(h * scale)
                pil_image = pil_image.resize((new_w, new_h), Image.Resampling.LANCZOS)
            
            # Compute image embedding
            image_embedding = self.compute_clip_embedding(pil_image)
            
            # Compute text embeddings cho c√°c classes
            if self.clip_type == 'ultralytics':
                # Ultralytics CLIP
                text_tokens = self.clip_model.tokenize(class_names)
                text_embeddings = self.clip_model.encode_text(text_tokens)
            elif self.clip_type == 'openai' and CLIP_AVAILABLE:
                # OpenAI CLIP
                text_tokens = clip.tokenize(class_names).to(self.device)
                with torch.no_grad():
                    text_embeddings = self.clip_model.encode_text(text_tokens)
            else:
                logger.error("[SAM+CLIP] CLIP kh√¥ng available ƒë·ªÉ encode text")
                return "unknown", 0.0
            
            # Normalize text embeddings
            text_embeddings = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)
            
            # Calculate similarities
            similarities = torch.cosine_similarity(
                image_embedding.unsqueeze(0),
                text_embeddings
            )
            
            # Find best match
            best_idx = similarities.argmax().item()
            best_class = class_names[best_idx]
            best_confidence = float(similarities[best_idx].item())
            
            # Ch·ªâ return n·∫øu confidence ƒë·ªß cao
            if best_confidence < 0.15:  # Threshold t·ªëi thi·ªÉu
                return "unknown", best_confidence
            
            return best_class, best_confidence
            
        except Exception as e:
            logger.warning(f"[SAM+CLIP] Error classifying object: {str(e)}")
            import traceback
            traceback.print_exc()
            return "unknown", 0.0
    
    def detect_all_objects(
        self,
        target_image: Union[str, Path, np.ndarray],
        min_area: Optional[int] = None,
        max_objects: Optional[int] = None,
        classify_objects: bool = True,
        custom_classes: Optional[List[str]] = None
    ) -> List[Dict]:
        """
        Detect T·∫§T C·∫¢ objects trong ·∫£nh (kh√¥ng c·∫ßn reference image)
        
        Args:
            target_image: ·∫¢nh c·∫ßn detect
            min_area: Di·ªán t√≠ch t·ªëi thi·ªÉu c·ªßa object (pixels), None = auto
            max_objects: S·ªë l∆∞·ª£ng objects t·ªëi ƒëa ƒë·ªÉ return, None = kh√¥ng gi·ªõi h·∫°n
            classify_objects: C√≥ ph√¢n lo·∫°i v√† ƒë·∫∑t t√™n objects kh√¥ng
            custom_classes: List c√°c class names t√πy ch·ªânh ƒë·ªÉ ph√¢n lo·∫°i
        
        Returns:
            List of detected objects v·ªõi format:
            {
                'mask': np.ndarray,
                'bbox': [x, y, w, h],
                'area': int,
                'confidence': float,
                'segment_id': int,
                'class_name': str,  # T√™n object (n·∫øu classify_objects=True)
                'class_confidence': float  # Confidence c·ªßa classification (n·∫øu classify_objects=True)
            }
        """
        # Segment t·∫•t c·∫£ objects
        segments = self.segment_image(target_image)
        
        if len(segments) == 0:
            logger.warning("[SAM+CLIP] Kh√¥ng t√¨m th·∫•y objects n√†o")
            return []
        
        # Filter by min_area n·∫øu c√≥
        if min_area is not None:
            segments = [s for s in segments if s['area'] >= min_area]
        
        # Sort by area (largest first)
        segments.sort(key=lambda x: x['area'], reverse=True)
        
        # Limit s·ªë l∆∞·ª£ng n·∫øu c√≥
        if max_objects is not None:
            segments = segments[:max_objects]
        
        # Classify objects n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu
        if classify_objects:
            if self.clip_model is None:
                logger.warning("[SAM+CLIP] CLIP model kh√¥ng available, kh√¥ng th·ªÉ ph√¢n lo·∫°i objects")
                logger.warning("[SAM+CLIP] Set classify_objects=False ho·∫∑c c√†i ƒë·∫∑t CLIP")
                for segment in segments:
                    segment['class_name'] = "object"
                    segment['class_confidence'] = 0.0
            else:
                # Load image ƒë·ªÉ crop segments
                numpy_image, _ = self._load_image(target_image)
                
                logger.info(f"[SAM+CLIP] Classifying {len(segments)} objects...")
                classified_count = 0
                for i, segment in enumerate(segments):
                    try:
                        # Crop segment t·ª´ image
                        x, y, w, h = segment['bbox']
                        x_max = min(x + w, numpy_image.shape[1])
                        y_max = min(y + h, numpy_image.shape[0])
                        
                        # ƒê·∫£m b·∫£o c√≥ k√≠ch th∆∞·ªõc h·ª£p l·ªá
                        if x_max <= x or y_max <= y:
                            segment['class_name'] = "unknown"
                            segment['class_confidence'] = 0.0
                            continue
                        
                        segment_crop = numpy_image[y:y_max, x:x_max]
                        
                        if segment_crop.size == 0:
                            segment['class_name'] = "unknown"
                            segment['class_confidence'] = 0.0
                            continue
                        
                        # Classify object
                        class_name, class_confidence = self.classify_object(
                            segment_crop,
                            class_names=custom_classes
                        )
                        
                        segment['class_name'] = class_name
                        segment['class_confidence'] = class_confidence
                        
                        if class_name != "unknown":
                            classified_count += 1
                            logger.debug(f"  Object {i+1}: {class_name} (confidence: {class_confidence:.2f})")
                        
                        if (i + 1) % 10 == 0:
                            logger.info(f"  Classified {i + 1}/{len(segments)} objects...")
                            
                    except Exception as e:
                        logger.warning(f"[SAM+CLIP] Error classifying segment {i}: {str(e)}")
                        import traceback
                        traceback.print_exc()
                        segment['class_name'] = "unknown"
                        segment['class_confidence'] = 0.0
                
                logger.info(f"[SAM+CLIP] ‚úÖ Successfully classified {classified_count}/{len(segments)} objects")
        else:
            # Kh√¥ng classify, set default values
            logger.info("[SAM+CLIP] Classification disabled")
            for segment in segments:
                segment['class_name'] = "object"
                segment['class_confidence'] = 0.0
        
        logger.info(f"[SAM+CLIP] Detected {len(segments)} objects (all objects mode)")
        
        # Log summary c·ªßa classification
        if classify_objects:
            class_counts = {}
            for seg in segments:
                class_name = seg.get('class_name', 'unknown')
                if class_name not in class_counts:
                    class_counts[class_name] = 0
                class_counts[class_name] += 1
            
            logger.info(f"[SAM+CLIP] Classification summary:")
            for class_name, count in sorted(class_counts.items(), key=lambda x: x[1], reverse=True):
                logger.info(f"  - {class_name}: {count}")
        
        return segments
    
    def detect_objects(
        self,
        target_image: Union[str, Path, np.ndarray],
        reference_image: Optional[Union[str, Path, np.ndarray, Image.Image]] = None,
        reference_name: Optional[str] = None,
        similarity_threshold: Optional[float] = None
    ) -> List[Dict]:
        """
        Detect objects trong target image d·ª±a tr√™n reference image
        
        Args:
            target_image: ·∫¢nh c·∫ßn detect objects
            reference_image: ·∫¢nh m·∫´u c·ªßa object c·∫ßn t√¨m (n·∫øu ch∆∞a register)
            reference_name: T√™n reference ƒë√£ register (n·∫øu ƒë√£ register tr∆∞·ªõc)
            similarity_threshold: Ng∆∞·ª°ng similarity (override default)
        
        Returns:
            List of detected objects v·ªõi format:
            {
                'mask': np.ndarray,
                'bbox': [x, y, w, h],
                'area': int,
                'similarity': float,  # Similarity v·ªõi reference
                'confidence': float
            }
        """
        if similarity_threshold is None:
            similarity_threshold = self.similarity_threshold
        
        # Get reference embedding
        if reference_name and reference_name in self.reference_embeddings:
            reference_embedding = self.reference_embeddings[reference_name]
        elif reference_image is not None:
            reference_embedding = self.compute_clip_embedding(reference_image)
        else:
            raise ValueError("C·∫ßn cung c·∫•p reference_image ho·∫∑c reference_name ƒë√£ register")
        
        # Segment target image
        segments = self.segment_image(target_image)
        
        if len(segments) == 0:
            logger.warning("[SAM+CLIP] Kh√¥ng t√¨m th·∫•y segments n√†o")
            return []
        
        # Load target image ƒë·ªÉ crop segments
        numpy_image, pil_image = self._load_image(target_image)
        
        # Compute embeddings cho m·ªói segment v√† match v·ªõi reference
        detected_objects = []
        for segment in segments:
            try:
                # Crop segment t·ª´ image
                x, y, w, h = segment['bbox']
                x_max = min(x + w, numpy_image.shape[1])
                y_max = min(y + h, numpy_image.shape[0])
                
                segment_crop = numpy_image[y:y_max, x:x_max]
                
                if segment_crop.size == 0:
                    continue
                
                # Compute CLIP embedding c·ªßa segment
                segment_embedding = self.compute_clip_embedding(segment_crop)
                
                # Calculate cosine similarity
                similarity = float(torch.cosine_similarity(
                    reference_embedding.unsqueeze(0),
                    segment_embedding.unsqueeze(0)
                ).item())
                
                # Filter by threshold
                if similarity >= similarity_threshold:
                    detected_objects.append({
                        'mask': segment['mask'],
                        'bbox': segment['bbox'],
                        'area': segment['area'],
                        'similarity': similarity,
                        'confidence': similarity,  # Use similarity as confidence
                        'segment_id': segment.get('segment_id', 0)
                    })
            
            except Exception as e:
                logger.warning(f"[SAM+CLIP] Error processing segment: {str(e)}")
                continue
        
        # Sort by similarity (highest first)
        detected_objects.sort(key=lambda x: x['similarity'], reverse=True)
        
        logger.info(f"[SAM+CLIP] Detected {len(detected_objects)} objects v·ªõi similarity >= {similarity_threshold}")
        
        return detected_objects
    
    def visualize_detections(
        self,
        image: Union[str, Path, np.ndarray],
        detections: List[Dict],
        output_path: Optional[Union[str, Path]] = None,
        show: bool = True,
        show_similarity: bool = True
    ) -> np.ndarray:
        """
        V·∫Ω detections l√™n ·∫£nh
        
        Args:
            image: Original image
            detections: List of detections t·ª´ detect_objects() ho·∫∑c detect_all_objects()
            output_path: Path ƒë·ªÉ save ·∫£nh (optional)
            show: C√≥ hi·ªÉn th·ªã ·∫£nh kh√¥ng
            show_similarity: C√≥ hi·ªÉn th·ªã similarity score kh√¥ng (ch·ªâ khi c√≥)
        
        Returns:
            Annotated image (numpy array)
        """
        numpy_image, _ = self._load_image(image)
        annotated = numpy_image.copy()
        
        # Generate colors for different objects
        colors = [
            (0, 255, 0),    # Green
            (255, 0, 0),    # Blue
            (0, 0, 255),    # Red
            (255, 255, 0),  # Cyan
            (255, 0, 255),  # Magenta
            (0, 255, 255),  # Yellow
        ]
        
        # Draw each detection
        for i, det in enumerate(detections):
            x, y, w, h = det['bbox']
            color = colors[i % len(colors)]
            
            # Draw bounding box
            cv2.rectangle(annotated, (x, y), (x + w, y + h), color, 2)
            
            # Draw mask overlay (semi-transparent)
            mask = det['mask']
            if mask.shape[:2] == annotated.shape[:2]:
                mask_resized = cv2.resize(mask, (w, h))
                mask_binary = (mask_resized > 0).astype(np.uint8)
                overlay = annotated.copy()
                overlay[y:y+h, x:x+w][mask_binary > 0] = color
                annotated = cv2.addWeighted(annotated, 0.7, overlay, 0.3, 0)
            
            # Draw label - ∆∞u ti√™n hi·ªÉn th·ªã class_name
            label = f"Object {i+1}"
            if 'class_name' in det:
                class_name = det['class_name']
                class_conf = det.get('class_confidence', 0.0)
                if class_name != "unknown" and class_name != "object" and class_conf > 0.15:
                    # Hi·ªÉn th·ªã t√™n class n·∫øu c√≥ v√† confidence ƒë·ªß cao
                    label = f"{class_name} ({class_conf:.2f})"
                elif show_similarity and 'similarity' in det:
                    label = f"Obj {i+1}: {det['similarity']:.2f}"
            elif show_similarity and 'similarity' in det:
                label = f"Obj {i+1}: {det['similarity']:.2f}"
            
            # Background for text
            (text_width, text_height), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)
            cv2.rectangle(annotated, (x, y - text_height - 10), (x + text_width, y), color, -1)
            cv2.putText(annotated, label, (x, y - 5),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # Save if requested
        if output_path:
            cv2.imwrite(str(output_path), annotated)
            logger.info(f"[SAM+CLIP] Saved visualization to: {output_path}")
        
        # Show if requested
        if show:
            cv2.imshow("SAM+CLIP Detections", annotated)
            cv2.waitKey(0)
            cv2.destroyAllWindows()
        
        return annotated
    
    def process_video(
        self,
        video_path: Union[str, Path],
        output_path: Optional[Union[str, Path]] = None,
        classify_objects: bool = True,
        custom_classes: Optional[List[str]] = None,
        frame_skip: int = 1,
        show_video: bool = True,
        save_video: bool = True
    ) -> Dict:
        """
        X·ª≠ l√Ω video ƒë·ªÉ detect v√† classify objects trong t·ª´ng frame
        
        Args:
            video_path: Path ƒë·∫øn video file
            output_path: Path ƒë·ªÉ save video output (optional)
            classify_objects: C√≥ ph√¢n lo·∫°i objects kh√¥ng
            custom_classes: List c√°c class names t√πy ch·ªânh
            frame_skip: X·ª≠ l√Ω m·ªói N frames (1 = t·∫•t c·∫£ frames, 2 = m·ªói 2 frames, ...)
            show_video: C√≥ hi·ªÉn th·ªã video trong qu√° tr√¨nh x·ª≠ l√Ω kh√¥ng
            save_video: C√≥ save video output kh√¥ng
        
        Returns:
            Dict v·ªõi th√¥ng tin:
            {
                'total_frames': int,
                'processed_frames': int,
                'detections_per_frame': List[List[Dict]],
                'object_counts': Dict[str, int],  # S·ªë l∆∞·ª£ng m·ªói lo·∫°i object
                'output_path': str
            }
        """
        import cv2
        
        # Load video
        video_path = Path(video_path)
        if not video_path.exists():
            # Th·ª≠ t√¨m trong test SAM directory
            test_sam_dir = Path(__file__).parent if '__file__' in globals() else Path.cwd() / "test SAM"
            test_path = test_sam_dir / video_path
            if test_path.exists():
                video_path = test_path
            else:
                raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y video: {video_path}")
        
        cap = cv2.VideoCapture(str(video_path))
        if not cap.isOpened():
            raise ValueError(f"Kh√¥ng th·ªÉ m·ªü video: {video_path}")
        
        # Get video properties
        fps = cap.get(cv2.CAP_PROP_FPS)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        logger.info(f"[SAM+CLIP] Video info: {width}x{height}, {fps:.2f} FPS, {total_frames} frames")
        
        # Setup video writer n·∫øu c·∫ßn save
        video_writer = None
        if save_video and output_path:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            video_writer = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))
            logger.info(f"[SAM+CLIP] Will save output to: {output_path}")
        
        # Statistics
        frame_count = 0
        processed_count = 0
        detections_per_frame = []
        object_counts = {}
        
        logger.info(f"[SAM+CLIP] B·∫Øt ƒë·∫ßu x·ª≠ l√Ω video (frame_skip={frame_skip})...")
        
        try:
            while cap.isOpened():
                ret, frame = cap.read()
                if not ret:
                    break
                
                frame_count += 1
                
                # Skip frames n·∫øu c·∫ßn
                if frame_count % frame_skip != 0:
                    # V·∫´n save frame g·ªëc n·∫øu kh√¥ng process
                    if video_writer:
                        video_writer.write(frame)
                    if show_video:
                        cv2.imshow("Video Processing", frame)
                        if cv2.waitKey(1) & 0xFF == ord('q'):
                            break
                    continue
                
                processed_count += 1
                
                # Detect objects trong frame
                detections = self.detect_all_objects(
                    target_image=frame,
                    classify_objects=classify_objects,
                    custom_classes=custom_classes,
                    min_area=1000,  # Filter small objects
                    max_objects=50  # Limit s·ªë l∆∞·ª£ng objects
                )
                
                # Update statistics
                detections_per_frame.append(detections)
                for det in detections:
                    class_name = det.get('class_name', 'unknown')
                    if class_name not in object_counts:
                        object_counts[class_name] = 0
                    object_counts[class_name] += 1
                
                # Visualize detections
                annotated_frame = self.visualize_detections(
                    image=frame,
                    detections=detections,
                    show=False,
                    show_similarity=False
                )
                
                # Save frame
                if video_writer:
                    video_writer.write(annotated_frame)
                
                # Show frame
                if show_video:
                    cv2.imshow("Video Processing", annotated_frame)
                    key = cv2.waitKey(1) & 0xFF
                    if key == ord('q') or key == 27:
                        logger.info("[SAM+CLIP] Ng∆∞·ªùi d√πng d·ª´ng x·ª≠ l√Ω")
                        break
                
                # Log progress
                if processed_count % 30 == 0:
                    logger.info(f"[SAM+CLIP] Processed {processed_count} frames ({frame_count}/{total_frames})...")
        
        finally:
            cap.release()
            if video_writer:
                video_writer.release()
            if show_video:
                cv2.destroyAllWindows()
        
        logger.info(f"[SAM+CLIP] ‚úÖ Ho√†n th√†nh x·ª≠ l√Ω video")
        logger.info(f"[SAM+CLIP]    Processed {processed_count}/{total_frames} frames")
        logger.info(f"[SAM+CLIP]    Object counts:")
        for class_name, count in sorted(object_counts.items(), key=lambda x: x[1], reverse=True):
            logger.info(f"      - {class_name}: {count}")
        
        return {
            'total_frames': total_frames,
            'processed_frames': processed_count,
            'detections_per_frame': detections_per_frame,
            'object_counts': object_counts,
            'output_path': str(output_path) if output_path else None
        }


def process_video_sam_clip(
    video_path: Union[str, Path],
    output_path: Optional[Union[str, Path]] = None,
    sam_model: str = "sam_b.pt",
    use_fastsam: bool = True,
    classify_objects: bool = True,
    custom_classes: Optional[List[str]] = None,
    frame_skip: int = 5,
    show_video: bool = True,
    save_video: bool = True,
    device: Optional[str] = None
) -> Dict:
    """
    X·ª≠ l√Ω video ƒë·ªÉ detect v√† classify objects
    
    Args:
        video_path: Path ƒë·∫øn video file
        output_path: Path ƒë·ªÉ save video output
        sam_model: SAM model name
        use_fastsam: S·ª≠ d·ª•ng FastSAM (nhanh h∆°n)
        classify_objects: C√≥ ph√¢n lo·∫°i objects kh√¥ng
        custom_classes: List c√°c class names t√πy ch·ªânh
        frame_skip: X·ª≠ l√Ω m·ªói N frames (1 = t·∫•t c·∫£, 5 = m·ªói 5 frames)
        show_video: C√≥ hi·ªÉn th·ªã video kh√¥ng
        save_video: C√≥ save video output kh√¥ng
        device: Device ('cuda', 'cpu', ho·∫∑c None)
    
    Returns:
        Dict v·ªõi th√¥ng tin x·ª≠ l√Ω
    
    Example:
        >>> result = process_video_sam_clip(
        ...     video_path="test_video.mp4",
        ...     output_path="result.mp4",
        ...     frame_skip=5
        ... )
        >>> print(f"Processed {result['processed_frames']} frames")
    """
    # Initialize detector
    detector = SAMCLIPDetector(
        sam_model=sam_model,
        use_fastsam=use_fastsam,
        device=device
    )
    
    # Process video
    result = detector.process_video(
        video_path=video_path,
        output_path=output_path,
        classify_objects=classify_objects,
        custom_classes=custom_classes,
        frame_skip=frame_skip,
        show_video=show_video,
        save_video=save_video
    )
    
    return result


def detect_all_objects_sam(
    target_image_path: Union[str, Path],
    output_path: Optional[Union[str, Path]] = None,
    sam_model: str = "sam_b.pt",
    use_fastsam: bool = False,
    min_area: Optional[int] = None,
    max_objects: Optional[int] = None,
    classify_objects: bool = True,
    custom_classes: Optional[List[str]] = None,
    device: Optional[str] = None
) -> List[Dict]:
    """
    Detect T·∫§T C·∫¢ objects trong ·∫£nh v·ªõi SAM (kh√¥ng c·∫ßn reference image)
    
    Args:
        target_image_path: Path ƒë·∫øn ·∫£nh c·∫ßn detect
        output_path: Path ƒë·ªÉ save ·∫£nh k·∫øt qu·∫£ (optional)
        sam_model: SAM model name
        use_fastsam: S·ª≠ d·ª•ng FastSAM
        min_area: Di·ªán t√≠ch t·ªëi thi·ªÉu (pixels)
        max_objects: S·ªë l∆∞·ª£ng objects t·ªëi ƒëa
        classify_objects: C√≥ ph√¢n lo·∫°i v√† ƒë·∫∑t t√™n objects kh√¥ng
        custom_classes: List c√°c class names t√πy ch·ªânh
        device: Device ('cuda', 'cpu', ho·∫∑c None)
    
    Returns:
        List of detected objects v·ªõi class_name v√† class_confidence
    
    Example:
        >>> detections = detect_all_objects_sam(
        ...     target_image_path="test_image.jpg",
        ...     output_path="result.jpg",
        ...     classify_objects=True
        ... )
        >>> print(f"Found {len(detections)} objects")
        >>> for det in detections:
        ...     print(f"  - {det['class_name']}: {det['class_confidence']:.2f}")
    """
    # Initialize detector
    detector = SAMCLIPDetector(
        sam_model=sam_model,
        use_fastsam=use_fastsam,
        device=device
    )
    
    # Detect all objects
    detections = detector.detect_all_objects(
        target_image=target_image_path,
        min_area=min_area,
        max_objects=max_objects,
        classify_objects=classify_objects,
        custom_classes=custom_classes
    )
    
    # Visualize
    if len(detections) > 0:
        detector.visualize_detections(
            image=target_image_path,
            detections=detections,
            output_path=output_path,
            show=True
        )
    else:
        logger.warning("[SAM+CLIP] Kh√¥ng t√¨m th·∫•y objects n√†o")
    
    return detections


def test_sam_clip_detection(
    reference_image_path: Union[str, Path],
    target_image_path: Union[str, Path],
    output_path: Optional[Union[str, Path]] = None,
    sam_model: str = "sam_b.pt",
    use_fastsam: bool = False,
    similarity_threshold: float = 0.25,
    device: Optional[str] = None
) -> List[Dict]:
    """
    H√†m test ƒë∆°n gi·∫£n ƒë·ªÉ detect objects v·ªõi SAM + CLIP
    
    Args:
        reference_image_path: Path ƒë·∫øn ·∫£nh m·∫´u (v√≠ d·ª•: ·∫£nh c√¢y b√∫t)
        target_image_path: Path ƒë·∫øn ·∫£nh c·∫ßn detect
        output_path: Path ƒë·ªÉ save ·∫£nh k·∫øt qu·∫£ (optional)
        sam_model: SAM model name
        use_fastsam: S·ª≠ d·ª•ng FastSAM
        similarity_threshold: Ng∆∞·ª°ng similarity
        device: Device ('cuda', 'cpu', ho·∫∑c None)
    
    Returns:
        List of detected objects
    
    Example:
        >>> detections = test_sam_clip_detection(
        ...     reference_image_path="pen_sample.jpg",
        ...     target_image_path="test_image.jpg",
        ...     output_path="result.jpg"
        ... )
        >>> print(f"Found {len(detections)} matches")
    """
    # Initialize detector
    detector = SAMCLIPDetector(
        sam_model=sam_model,
        use_fastsam=use_fastsam,
        similarity_threshold=similarity_threshold,
        device=device
    )
    
    # Detect objects
    detections = detector.detect_objects(
        target_image=target_image_path,
        reference_image=reference_image_path
    )
    
    # Visualize
    if len(detections) > 0:
        detector.visualize_detections(
            image=target_image_path,
            detections=detections,
            output_path=output_path,
            show=True
        )
    else:
        logger.warning("[SAM+CLIP] Kh√¥ng t√¨m th·∫•y objects n√†o")
    
    return detections


if __name__ == "__main__":
    """
    Script ƒë·ªÉ test tr·ª±c ti·∫øp
    
    Usage:
        python test_sam_clip.py --reference pen_sample.jpg --target test_image.jpg --output result.jpg
    """
    import argparse
    
    parser = argparse.ArgumentParser(description="Test SAM + CLIP Object Detection")
    parser.add_argument("--reference", type=str, default=None, help="Path to reference image (optional, n·∫øu kh√¥ng c√≥ s·∫Ω detect t·∫•t c·∫£)")
    parser.add_argument("--target", type=str, required=True, help="Path to target image")
    parser.add_argument("--output", type=str, default=None, help="Path to save result image")
    parser.add_argument("--sam-model", type=str, default="sam_b.pt", help="SAM model name")
    parser.add_argument("--fastsam", action="store_true", help="Use FastSAM instead of SAM")
    parser.add_argument("--threshold", type=float, default=0.25, help="Similarity threshold (ch·ªâ d√πng khi c√≥ --reference)")
    parser.add_argument("--device", type=str, default=None, help="Device (cuda/cpu)")
    parser.add_argument("--min-area", type=int, default=None, help="Minimum area in pixels (ch·ªâ d√πng khi detect all)")
    parser.add_argument("--max-objects", type=int, default=None, help="Maximum number of objects (ch·ªâ d√πng khi detect all)")
    parser.add_argument("--all", action="store_true", help="Detect all objects (kh√¥ng c·∫ßn reference image)")
    parser.add_argument("--no-classify", action="store_true", help="Kh√¥ng ph√¢n lo·∫°i objects (nhanh h∆°n nh∆∞ng kh√¥ng c√≥ t√™n)")
    parser.add_argument("--classes", type=str, default=None, help="Custom classes (comma-separated), v√≠ d·ª•: 'pen,book,cup'")
    parser.add_argument("--video", action="store_true", help="X·ª≠ l√Ω video thay v√¨ ·∫£nh")
    parser.add_argument("--frame-skip", type=int, default=5, help="X·ª≠ l√Ω m·ªói N frames khi x·ª≠ l√Ω video (default: 5)")
    parser.add_argument("--no-show", action="store_true", help="Kh√¥ng hi·ªÉn th·ªã video trong qu√° tr√¨nh x·ª≠ l√Ω")
    parser.add_argument("--no-save", action="store_true", help="Kh√¥ng save video output")
    
    args = parser.parse_args()
    
    # Setup logging
    logging.basicConfig(level=logging.INFO)
    
    # Check if processing video
    if args.video:
        logger.info("üé• Processing VIDEO mode")
        result = process_video_sam_clip(
            video_path=args.target,
            output_path=args.output,
            sam_model=args.sam_model,
            use_fastsam=args.fastsam,
            classify_objects=not args.no_classify,
            custom_classes=[c.strip() for c in args.classes.split(',')] if args.classes else None,
            frame_skip=args.frame_skip,
            show_video=not args.no_show,
            save_video=not args.no_save,
            device=args.device
        )
        
        print(f"\n‚úÖ Video processing completed!")
        print(f"  Processed: {result['processed_frames']}/{result['total_frames']} frames")
        print(f"  Object counts:")
        for class_name, count in sorted(result['object_counts'].items(), key=lambda x: x[1], reverse=True):
            print(f"    - {class_name}: {count}")
        if result['output_path']:
            print(f"  Output saved to: {result['output_path']}")
        import sys
        sys.exit(0)
    
    # Run test for images
    if args.all or args.reference is None:
        # Detect all objects mode
        logger.info("üîç Detecting ALL objects (no reference image needed)")
        
        # Parse custom classes n·∫øu c√≥
        custom_classes = None
        if args.classes:
            custom_classes = [c.strip() for c in args.classes.split(',')]
            logger.info(f"Using custom classes: {custom_classes}")
        
        detections = detect_all_objects_sam(
            target_image_path=args.target,
            output_path=args.output,
            sam_model=args.sam_model,
            use_fastsam=args.fastsam,
            min_area=args.min_area,
            max_objects=args.max_objects,
            classify_objects=not args.no_classify,
            custom_classes=custom_classes,
            device=args.device
        )
        
        print(f"\n‚úÖ Detected {len(detections)} objects")
        for i, det in enumerate(detections):
            if 'class_name' in det and det['class_name'] != "unknown" and det['class_name'] != "object":
                print(f"  {i+1}. {det['class_name']} (confidence: {det.get('class_confidence', 0):.2f}), Area: {det['area']} pixels")
            else:
                print(f"  {i+1}. Area: {det['area']} pixels, BBox: {det['bbox']}")
    else:
        # Reference-based detection mode
        logger.info("üîç Detecting objects matching reference image")
        detections = test_sam_clip_detection(
            reference_image_path=args.reference,
            target_image_path=args.target,
            output_path=args.output,
            sam_model=args.sam_model,
            use_fastsam=args.fastsam,
            similarity_threshold=args.threshold,
            device=args.device
        )
        
        print(f"\n‚úÖ Detected {len(detections)} objects")
        for i, det in enumerate(detections):
            print(f"  {i+1}. Similarity: {det['similarity']:.3f}, Area: {det['area']} pixels")

